<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h3 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    p {
        text-align: justify;
    }
</style>

<html>

<head>
    <title>Controllable Representation Learning</title>
    <!-- <meta property="og:image" content="./asset/splash.png" /> -->
    <meta property="og:title" content="Controllable Representation Learning" />
</head>

<body>
    <br>
	<p>Representation Learning from Noisy-labeled Data</p>
    <center>
        <span style="font-size:26px"><strong>Coupled-View Deep Classifier Learning from Multiple Noisy Annotators</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="#" target="_blank">Shikun Li</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html">Shiming Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Chunhui Zhang</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Hao Wen</a></span> &emsp;
					<span style="font-size:20px"><a href="#">Tengfei Liu</a></span> &emsp;
					<span style="font-size:20px"><a href="#">Weiqiang Wang</a></span> &emsp;
                </td>
            </tr>
        </table>

        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://aaai.org/ojs/index.php/AAAI/article/view/5898'> Paper [AAAI 2020]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/li2020coupled.webp" height="300px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Typically, learning a deep classifier from massive cleanly annotated instances is effective but impractical in many real-
            world scenarios. An alternative is collecting and aggregating multiple noisy annotations for each instance to train the
			classifier. Inspired by that, this paper proposes to learn deep classifier from multiple noisy annotators via a coupled-view
			learning approach, where the learning view from data is represented by deep neural networks for data classification and
			the learning view from labels is described by a Naive Bayes classifier for label aggregation.Such coupled-view learning is
			converted to a supervised learning problem under the mutual supervision of the aggregated and predicted labels, and can be
			solved via alternate optimization to update labels and refine the classifiers. To alleviate the propagation of incorrect labels,
			small-loss metric is proposed to select reliable instances in both views. A co-teaching strategy with class-weighted loss
			is further leveraged in the deep classifier learning, which uses two networks with different learning abilities to teach each
			other, and the diverse errors introduced by noisy labels can be filtered out by peer networks. By these strategies, our 
			approach can finally learn a robust data classifier which less overfits to label noise. Experimental results on synthetic and
			real data demonstrate the effectiveness and robustness of the proposed approach.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Li2020CVL,
  title     = {Coupled-view Deep Classifier Learning from Multiple Noisy Annotators},
  author    = {Shikun Li and Shiming Ge and Yingying Hua and Chunhui Zhang and Hao Wen and Tengfei Liu and Weiqiang Wang},
  booktitle = {The 34th AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA, February 7-12, 2020},
  pages     = {4667--4674},
  year      = {2020}
}        
</span>
        </pre>
    </table>
    <hr>

    <br>
	<p>Representation Learning Against Adversarial Examples</p>
    <center>
        <span style="font-size:26px"><strong>Defending Against Adversarial Examples via Soft Decision Trees Embedding</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
				    <span style="font-size:20px"><a href="#">Yingying Hua</a></span> &emsp;
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xindi Gao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xin Jin</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Dan Zeng</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://dl.acm.org/doi/10.1145/3343031.3351012'> Paper
                            [ACM MM 2019]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/Hua2019SDTE.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Convolutional neural networks (CNNs) have shown vulnerable to adversarial examples which contain imperceptible perturbations.
			In this paper, we propose an approach to defend against adversarial examples with soft decision trees embedding. Firstly, we extract the
			semantic features of adversarial examples with a feature extraction network. Then, a specific soft decision tree is trained and embedded
			to select the key semantic features for each feature map from convolutional layers and the selected features are fed to a light-weight
			classification network. To this end, we use the probability distributions of each tree node to quantify the semantic features. In this
			way, some small perturbations can be effectively removed and the selected features are more discriminative in identifying adversarial
			examples. Moreover, the influence of adversarial perturbations on classification can be reduced by migrating the interpretability of
			soft decision trees into the black-box neural networks. We conduct experiments to defend the state-of-the-art adversarial attacks. The
			experimental results demonstrate that our proposed approach can effectively defend against these attacks and improve the robustness
			of deep neural networks.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Hua2019SDTE,
  title     = {Defending Against Adversarial Examples via Soft Decision Trees Embedding},
  author    = {Yingying Hua and Shiming Ge and Xindi Gao and Xin Jin and Dan Zeng},
  booktitle = {The 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019},
  pages     = {2106--2114},
  year      = {2019}
}
</span>
        </pre>
    </table>
    <hr>
	
    <br>
	<p>Multi-Granularity Representation Learning</p>
    <center>
        <span style="font-size:26px"><strong>Receptive Multi-Granularity Representation for Person Re-Identification</strong></span><br>
        <br><br>

        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
				    <span style="font-size:20px"><a href="#">Guanshuo Wang</a></span> &emsp;	    
                    <span style="font-size:20px"><a href="#">Yufeng Yuan</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Jiwei Li</a></span> &emsp;
					<span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;	
                    <span style="font-size:20px"><a href="#">Xi Zhou</a></span> &emsp;
                </td>
            </tr>
        </table>


        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://ieeexplore.ieee.org/document/9075365'> Paper
                            [IEEE Trans]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/wang2020RMGL.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            A key for person re-identification is achieving consistent local details for discriminative representation across
			variable environments. Current stripe-based feature learning approaches have delivered impressive accuracy, but do not make
			a proper trade-off between diversity, locality, and robustness, which easily suffers from part semantic inconsistency for the
			conflict between rigid partition and misalignment. This paper proposes a receptive multi-granularity learning approach to
			facilitate stripe-based feature learning. This approach performs local partition on the intermediate representations to operate
			receptive region ranges, rather than current approaches on input images or output features, thus can enhance the representation 
			of locality while remaining proper local association. Toward this end, the local partitions are adaptively pooled by using 
			significance-balanced activations for uniform stripes. Random shifting augmentation is further introduced for a higher variance 
			of person appearing regions within bounding boxes to ease misalignment. By two-branch network architecture, different scales of 
			discriminative identity representation can be learned. In this way, our model can provide a more comprehensive and efficient 
			feature representation without larger model storage costs. Extensive experiments on intra-dataset and cross-dataset evaluations 
			demonstrate the effectiveness of the proposed approach. Especially, our approach achieves a stateof-the-art accuracy of 
			96.2%@Rank-1 or 90.0%@mAP on the challenging Market-1501 benchmark.</p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@article{wang2020RMGL,
  title   = {Receptive Multi-Granularity Representation for Person Re-Identification},
  author  = {Guanshuo Wang and Yufeng Yuan and Jiwei Li and Shiming Ge and Xi Zhou},
  journal = {IEEE Transactions on Image Processing},
  volume  = {29},
  pages   = {6096--6109},
  year    = {2020}
}
</span>
        </pre>
    </table>
    <hr>

    <br>
	<p>Representation Learning for Few-Shots Data</p>
    <center>
        <span style="font-size:26px"><strong>Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face Recognition in
                the
                Wild</strong></span><br>

        <br><br>
        <table align=center width=1000px>
            <tr>
                <td align=center width=1000px>
                    <span style="font-size:20px"><a href="../people/geshiming.html" target="_blank">Shiming
                            Ge</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Shengwei Zhao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Xindi Gao</a></span> &emsp;
                    <span style="font-size:20px"><a href="#">Jia Li</a></span> &emsp;
                </td>
            </tr>
        </table>
        <br>
        <br>

        <table align=center width=900px>
            <tr>
                <td align=center width=300px>
                    <span style="font-size:20px"><a href='https://dl.acm.org/doi/10.1145/3343031.3351082'> Paper [ACM
                            MM 2019]</a></span> &emsp;&emsp;
                    <!-- <span style="font-size:20px"><a href='http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html'> Blogpost</a></span> &emsp;&emsp;
		<span style="font-size:20px"><a href='https://github.com/yukimasano/self-label'> Code [PyTorch]</a></span> -->
                </td>
            </tr>
        </table>
    </center>
    <br>

    <table align=center width=800px>
        <tr>
            <td width=600px>
                <center>
                    <img class="rounded" src="../img/research2/conlearning/GeZG019.webp" height="400px"></img>
                    <br>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>Abstract</strong></h3>
        </center>
        <p>
            Is it possible to train an effective face recognition model with fewer shots that works efficiently on
            low-resolution faces in the wild? To answer this question, this paper proposes a few-shot knowledge
            distillation approach to learn an ultrafast face recognizer via two steps. In the first step, we initialize
            a simple yet effective face recognition model on synthetic low-resolution faces by distilling knowledge from
            an existing complex model. By removing the redundancies in both face images and the model structure, the
            initial model can provide an ultrafast speed with impressive recognition accuracy. To further adapt this
            model into the wild scenarios with fewer faces per person, the second step refines the model via few-shot
            learning by incorporating a relation module that compares low-resolution query faces with faces in the
            support set. In this manner, the performance of the model can be further enhanced with only fewer
            low-resolution faces in the wild. Experimental results show that the proposed approach performs favorably
            against state-of-the-arts in recognizing low-resolution faces with an extremely low memory of 30KB and runs
            at an ultrafast speed of 1,460 faces per second on CPU or 21,598 faces per second on GPU.
        </p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h3><strong>BibTex</strong></h3>
        </center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
@inproceedings{Ge2019FSLR,
    author    = {Shiming Ge and Shengwei Zhao and Xindi Gao and Jia Li},
    title     = {Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face Recognition in the Wild},
    booktitle = {The 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019},
    pages     = {229--237},
    year      = {2019},
}    
</span>
        </pre>
    </table>
    <hr>
    <br>
	<p>We are studying more works about Controllable Representation Learning, including Explainable Representation Learning, 
	Privacy Preserving Representation Learning, Fair Representation Learning, and Representation learning from Multi-source Heterogeneous Data!</p>

</body>

</html>